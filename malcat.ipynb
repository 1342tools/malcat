{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna xgboost scikit-learn pandas numpy matplotlib seaborn psutil"
      ],
      "metadata": {
        "id": "K9s_9qx6KST_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "XucKpw1iI5Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Jm33pBPGJ7XW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abih8Wb7I4b-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import joblib\n",
        "import time\n",
        "import psutil\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class SystemMonitor:\n",
        "    \"\"\"Monitor system resources during training\"\"\"\n",
        "    def __init__(self, interval=1.0):\n",
        "        self.interval = interval\n",
        "        self.cpu_percentages = []\n",
        "        self.memory_usage = []\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start monitoring\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.cpu_percentages = []\n",
        "        self.memory_usage = []\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Record current system stats\"\"\"\n",
        "        self.cpu_percentages.append(psutil.cpu_percent(interval=0.1))\n",
        "        self.memory_usage.append(psutil.Process().memory_info().rss / 1024 / 1024)  # MB\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop monitoring and return stats\"\"\"\n",
        "        self.end_time = time.time()\n",
        "        return {\n",
        "            'training_time_seconds': self.end_time - self.start_time,\n",
        "            'avg_cpu_percent': np.mean(self.cpu_percentages),\n",
        "            'max_cpu_percent': max(self.cpu_percentages),\n",
        "            'avg_memory_mb': np.mean(self.memory_usage),\n",
        "            'max_memory_mb': max(self.memory_usage)\n",
        "        }\n",
        "\n",
        "class ExtendedAPIEncoder:\n",
        "    \"\"\"\n",
        "    Custom encoder for API calls that handles unseen values using a predefined vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, unknown_value=-1):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.unknown_value = unknown_value\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def load_vocabulary(self, vocab_file):\n",
        "        \"\"\"Load API vocabulary from a text file\"\"\"\n",
        "        with open(vocab_file, 'r') as f:\n",
        "            api_calls = {line.strip() for line in f if line.strip()}\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def add_to_vocabulary(self, api_calls):\n",
        "        \"\"\"Add additional API calls to vocabulary\"\"\"\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def fit(self, api_calls):\n",
        "        \"\"\"Fit the encoder using both the vocabulary and training data\"\"\"\n",
        "        # Combine vocabulary with observed API calls\n",
        "        all_apis = list(self.vocabulary.union(set(api_calls)))\n",
        "        self.label_encoder.fit(all_apis)\n",
        "        return self\n",
        "\n",
        "    def transform(self, api_calls):\n",
        "        \"\"\"Transform API calls, handling unseen values gracefully\"\"\"\n",
        "        # Create a copy to avoid modifying the input\n",
        "        api_calls_clean = np.array(api_calls).copy()\n",
        "\n",
        "        # Replace unseen values with a special token\n",
        "        mask = ~np.isin(api_calls_clean, self.label_encoder.classes_)\n",
        "        if mask.any():\n",
        "            unseen_apis = set(api_calls_clean[mask])\n",
        "            print(f\"Warning: Found {len(unseen_apis)} unseen API calls not in vocabulary.\")\n",
        "            api_calls_clean[mask] = self.label_encoder.classes_[0]  # Use first class as unknown token\n",
        "\n",
        "        return self.label_encoder.transform(api_calls_clean)\n",
        "\n",
        "    def fit_transform(self, api_calls):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        self.fit(api_calls)\n",
        "        return self.transform(api_calls)\n",
        "\n",
        "    def inverse_transform(self, encoded_values):\n",
        "        \"\"\"Convert encoded values back to API calls\"\"\"\n",
        "        return self.label_encoder.inverse_transform(encoded_values)\n",
        "\n",
        "    def classes_(self):\n",
        "        \"\"\"Return the classes (API calls) known to the encoder\"\"\"\n",
        "        return self.label_encoder.classes_\n",
        "\n",
        "def load_and_combine_data(file_paths):\n",
        "    \"\"\"\n",
        "    Load and combine data from multiple CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): List of paths to CSV files containing malware data\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "    required_columns = ['first_api', 'last_api', 'api_call_count',\n",
        "                       'api_sequence', 'malware_type']\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Validate required columns\n",
        "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "            if missing_columns:\n",
        "                raise ValueError(f\"File {file_path} is missing required columns: {missing_columns}\")\n",
        "\n",
        "            # Basic data cleaning\n",
        "            df = df.dropna()\n",
        "            df['api_call_count'] = df['api_call_count'].astype(int)\n",
        "            df['api_sequence'] = df['api_sequence'].astype(str)\n",
        "\n",
        "            # Add source file information\n",
        "            df['source_file'] = os.path.basename(file_path)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "            print(f\"\\nLoaded data from {file_path}\")\n",
        "            print(f\"Samples: {len(df)}\")\n",
        "            print(\"Class distribution:\")\n",
        "            for malware_type, count in df['malware_type'].value_counts().items():\n",
        "                print(f\"{malware_type}: {count} samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Print combined dataset statistics\n",
        "    print(\"\\nCombined Dataset Statistics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Total samples: {len(combined_df)}\")\n",
        "    print(\"\\nOverall class distribution:\")\n",
        "    class_dist = combined_df['malware_type'].value_counts()\n",
        "    for malware_type, count in class_dist.items():\n",
        "        print(f\"{malware_type}: {count} samples\")\n",
        "\n",
        "    # API calls statistics\n",
        "    print(\"\\nAPI calls statistics:\")\n",
        "    api_stats = combined_df['api_call_count'].describe()\n",
        "    print(f\"Min API calls: {api_stats['min']:.0f}\")\n",
        "    print(f\"Max API calls: {api_stats['max']:.0f}\")\n",
        "    print(f\"Mean API calls: {api_stats['mean']:.0f}\")\n",
        "    print(f\"Median API calls: {api_stats['50%']:.0f}\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def prepare_data(df, api_vocab_file, max_features=2000, use_smote=True):\n",
        "    \"\"\"\n",
        "    Prepare the data using extended API vocabulary.\n",
        "    \"\"\"\n",
        "    # Create extended API encoders\n",
        "    first_api_encoder = ExtendedAPIEncoder()\n",
        "    last_api_encoder = ExtendedAPIEncoder()\n",
        "    malware_type_encoder = LabelEncoder()\n",
        "\n",
        "    # Load API vocabulary\n",
        "    print(\"Loading API vocabulary...\")\n",
        "    first_api_encoder.load_vocabulary(api_vocab_file)\n",
        "    last_api_encoder.load_vocabulary(api_vocab_file)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    print(\"Encoding API calls...\")\n",
        "    df['first_api_encoded'] = first_api_encoder.fit_transform(df['first_api'])\n",
        "    df['last_api_encoded'] = last_api_encoder.fit_transform(df['last_api'])\n",
        "    df['malware_type_encoded'] = malware_type_encoder.fit_transform(df['malware_type'])\n",
        "\n",
        "    # Normalize api_call_count\n",
        "    df['api_call_count_norm'] = np.log1p(df['api_call_count'])\n",
        "\n",
        "    # Modify TF-IDF to use the API vocabulary\n",
        "    print(\"Creating TF-IDF features...\")\n",
        "    with open(api_vocab_file, 'r') as f:\n",
        "        vocabulary = {line.strip() for line in f if line.strip()}\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        sublinear_tf=True,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=1,  # Changed to 1 to keep rare API calls\n",
        "        vocabulary=vocabulary  # Use predefined vocabulary\n",
        "    )\n",
        "    api_sequence_features = tfidf.fit_transform(df['api_sequence'])\n",
        "\n",
        "    # Create feature matrix\n",
        "    numeric_features = np.column_stack((\n",
        "        df['first_api_encoded'],\n",
        "        df['last_api_encoded'],\n",
        "        df['api_call_count_norm']\n",
        "    ))\n",
        "    numeric_features_sparse = csr_matrix(numeric_features)\n",
        "\n",
        "    # Combine features\n",
        "    X = hstack([numeric_features_sparse, api_sequence_features])\n",
        "    y = df['malware_type_encoded']\n",
        "\n",
        "    if use_smote:\n",
        "        print(\"\\nApplying SMOTE to balance classes...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "        print(f\"Shape after SMOTE: {X.shape}\")\n",
        "\n",
        "    feature_names = (['First API', 'Last API', 'API Call Count'] +\n",
        "                    [f'API_Seq_{i}' for i in range(api_sequence_features.shape[1])])\n",
        "\n",
        "    return (X, y, malware_type_encoder, first_api_encoder, last_api_encoder,\n",
        "            tfidf, feature_names)\n",
        "\n",
        "def format_classification_report(y_true, y_pred, target_names):\n",
        "    \"\"\"Generate formatted classification report with 4 decimal precision\"\"\"\n",
        "    report_dict = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
        "\n",
        "    formatted_report = {\"precision\": {}, \"recall\": {}, \"f1-score\": {}, \"support\": {}}\n",
        "\n",
        "    for class_name in target_names:\n",
        "        metrics = report_dict[class_name]\n",
        "        formatted_report[\"precision\"][class_name] = f\"{metrics['precision']:.4f}\"\n",
        "        formatted_report[\"recall\"][class_name] = f\"{metrics['recall']:.4f}\"\n",
        "        formatted_report[\"f1-score\"][class_name] = f\"{metrics['f1-score']:.4f}\"\n",
        "        formatted_report[\"support\"][class_name] = int(metrics['support'])\n",
        "\n",
        "    # Add weighted averages\n",
        "    for avg_type in ['macro avg', 'weighted avg']:\n",
        "        metrics = report_dict[avg_type]\n",
        "        formatted_report[\"precision\"][avg_type] = f\"{metrics['precision']:.4f}\"\n",
        "        formatted_report[\"recall\"][avg_type] = f\"{metrics['recall']:.4f}\"\n",
        "        formatted_report[\"f1-score\"][avg_type] = f\"{metrics['f1-score']:.4f}\"\n",
        "        formatted_report[\"support\"][avg_type] = int(metrics['support'])\n",
        "\n",
        "    return formatted_report\n",
        "\n",
        "def evaluate_model(model, X, y, malware_type_encoder, feature_names, output_dir, system_monitor):\n",
        "    \"\"\"\n",
        "    Enhanced evaluation with system metrics and precise formatting\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    class_names = malware_type_encoder.classes_\n",
        "\n",
        "    # Generate formatted classification report\n",
        "    formatted_report = format_classification_report(y_test, y_pred, class_names)\n",
        "\n",
        "    # Save metrics to JSON\n",
        "    metrics_file = os.path.join(output_dir, 'model_metrics.json')\n",
        "    system_stats = system_monitor.stop()\n",
        "\n",
        "    metrics_data = {\n",
        "        'classification_metrics': formatted_report,\n",
        "        'system_stats': system_stats,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "    # Print formatted results\n",
        "    print(\"\\nClassification Metrics (4 decimal precision):\")\n",
        "    print(\"-\" * 80)\n",
        "    for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
        "        print(f\"\\n{metric.upper()}:\")\n",
        "        for class_name, value in formatted_report[metric].items():\n",
        "            print(f\"{class_name}: {value}\")\n",
        "\n",
        "    print(\"\\nSystem Statistics:\")\n",
        "    print(f\"Total training time: {system_stats['training_time_seconds']:.2f} seconds\")\n",
        "    print(f\"Average CPU usage: {system_stats['avg_cpu_percent']:.1f}%\")\n",
        "    print(f\"Peak CPU usage: {system_stats['max_cpu_percent']:.1f}%\")\n",
        "    print(f\"Average memory usage: {system_stats['avg_memory_mb']:.1f} MB\")\n",
        "    print(f\"Peak memory usage: {system_stats['max_memory_mb']:.1f} MB\")\n",
        "\n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance plot\n",
        "    n_top_features = 20\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    })\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "    feature_importance = feature_importance.head(n_top_features)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "    plt.title(f'Top {n_top_features} Most Important Features')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'feature_importance.png'))\n",
        "    plt.close()\n",
        "\n",
        "def train_random_forest(X, y, n_estimators=100, n_jobs=-1, system_monitor=None):\n",
        "    \"\"\"\n",
        "    Enhanced Random Forest training with system monitoring\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        n_jobs=n_jobs,\n",
        "        random_state=42,\n",
        "        class_weight='balanced',\n",
        "        max_features='sqrt'\n",
        "    )\n",
        "\n",
        "    cv_scores = []\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        score = rf_model.score(X_val, y_val)\n",
        "        cv_scores.append(score)\n",
        "        print(f\"Fold {fold} accuracy: {score:.4f}\")\n",
        "\n",
        "        if system_monitor:\n",
        "            system_monitor.update()\n",
        "\n",
        "    print(f\"\\nMean CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Final training on full dataset\n",
        "    rf_model.fit(X, y)\n",
        "\n",
        "    return rf_model\n",
        "\n",
        "def save_artifacts(output_dir, model, tfidf, malware_type_encoder,\n",
        "                  first_api_encoder, last_api_encoder, feature_names):\n",
        "    \"\"\"\n",
        "    Save all model artifacts required for inference.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save model and preprocessors\n",
        "    artifact_dict = {\n",
        "        'malware_classifier.joblib': model,\n",
        "        'tfidf_vectorizer.joblib': tfidf,\n",
        "        'malware_type_encoder.joblib': malware_type_encoder,\n",
        "        'first_api_encoder.joblib': first_api_encoder,\n",
        "        'last_api_encoder.joblib': last_api_encoder,\n",
        "        'feature_names.joblib': feature_names\n",
        "    }\n",
        "\n",
        "    for filename, artifact in artifact_dict.items():\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        joblib.dump(artifact, filepath)\n",
        "        print(f\"Saved {filename} to {filepath}\")\n",
        "\n",
        "def main(csv_paths, output_dir, api_vocab_file):\n",
        "    \"\"\"\n",
        "    Enhanced main execution function with system monitoring\n",
        "    \"\"\"\n",
        "    system_monitor = SystemMonitor()\n",
        "    system_monitor.start()\n",
        "\n",
        "    # Load and combine data\n",
        "    df = load_and_combine_data(csv_paths)\n",
        "\n",
        "    # Prepare data\n",
        "    (X, y, malware_type_encoder, first_api_encoder, last_api_encoder,\n",
        "     tfidf, feature_names) = prepare_data(df, api_vocab_file, max_features=2000, use_smote=True)\n",
        "\n",
        "    # Train model with system monitoring\n",
        "    model = train_random_forest(X, y, n_estimators=200, system_monitor=system_monitor)\n",
        "\n",
        "    # Evaluate model with system metrics\n",
        "    evaluate_model(model, X, y, malware_type_encoder, feature_names, output_dir, system_monitor)\n",
        "\n",
        "    # Save artifacts\n",
        "    save_artifacts(\n",
        "        output_dir,\n",
        "        model,\n",
        "        tfidf,\n",
        "        malware_type_encoder,\n",
        "        first_api_encoder,\n",
        "        last_api_encoder,\n",
        "        feature_names\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining complete. All artifacts saved to {output_dir}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_paths = [\n",
        "        \"/csv/path/data.csv\"\n",
        "    ]\n",
        "    output_dir = \"output/directory/\"\n",
        "    api_vocab_file = \"/api/calls/path/windowsapicalls.txt\"\n",
        "    model = main(csv_paths, output_dir, api_vocab_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "3wwxCw51KD7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import joblib\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import psutil\n",
        "from datetime import datetime\n",
        "\n",
        "class ExtendedAPIEncoder:\n",
        "    \"\"\"\n",
        "    Custom encoder for API calls that handles unseen values using a predefined vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, unknown_value=-1):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.unknown_value = unknown_value\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def load_vocabulary(self, vocab_file):\n",
        "        \"\"\"Load API vocabulary from a text file\"\"\"\n",
        "        with open(vocab_file, 'r') as f:\n",
        "            api_calls = {line.strip() for line in f if line.strip()}\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def add_to_vocabulary(self, api_calls):\n",
        "        \"\"\"Add additional API calls to vocabulary\"\"\"\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def fit(self, api_calls):\n",
        "        \"\"\"Fit the encoder using both the vocabulary and training data\"\"\"\n",
        "        all_apis = list(self.vocabulary.union(set(api_calls)))\n",
        "        self.label_encoder.fit(all_apis)\n",
        "        return self\n",
        "\n",
        "    def transform(self, api_calls):\n",
        "        \"\"\"Transform API calls, handling unseen values gracefully\"\"\"\n",
        "        api_calls_clean = np.array(api_calls).copy()\n",
        "        mask = ~np.isin(api_calls_clean, self.label_encoder.classes_)\n",
        "        if mask.any():\n",
        "            unseen_apis = set(api_calls_clean[mask])\n",
        "            print(f\"Warning: Found {len(unseen_apis)} unseen API calls not in vocabulary.\")\n",
        "            api_calls_clean[mask] = self.label_encoder.classes_[0]\n",
        "\n",
        "        return self.label_encoder.transform(api_calls_clean)\n",
        "\n",
        "    def fit_transform(self, api_calls):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        self.fit(api_calls)\n",
        "        return self.transform(api_calls)\n",
        "\n",
        "    def inverse_transform(self, encoded_values):\n",
        "        \"\"\"Convert encoded values back to API calls\"\"\"\n",
        "        return self.label_encoder.inverse_transform(encoded_values)\n",
        "\n",
        "    def classes_(self):\n",
        "        \"\"\"Return the classes (API calls) known to the encoder\"\"\"\n",
        "        return self.label_encoder.classes_\n",
        "\n",
        "def get_system_metrics():\n",
        "    \"\"\"\n",
        "    Collect system metrics during model execution.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing system metrics\n",
        "    \"\"\"\n",
        "    process = psutil.Process()\n",
        "    return {\n",
        "        'memory_usage_mb': process.memory_info().rss / 1024 / 1024,\n",
        "        'cpu_percent': process.cpu_percent(),\n",
        "        'threads': process.num_threads(),\n",
        "    }\n",
        "\n",
        "def create_run_directory(base_output_dir):\n",
        "    \"\"\"\n",
        "    Create a timestamped directory for the current run.\n",
        "\n",
        "    Args:\n",
        "        base_output_dir (str): Base directory for all runs\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the newly created directory\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    run_dir = os.path.join(base_output_dir, f'run_{timestamp}')\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for organization\n",
        "    os.makedirs(os.path.join(run_dir, 'individual_results'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(run_dir, 'confusion_matrices'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(run_dir, 'combined_results'), exist_ok=True)\n",
        "\n",
        "    return run_dir\n",
        "\n",
        "def load_model_artifacts(model_dir):\n",
        "    \"\"\"\n",
        "    Load all saved model artifacts from the specified directory.\n",
        "\n",
        "    Args:\n",
        "        model_dir (str): Directory containing the saved model artifacts\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tfidf, malware_encoder, first_api_encoder, last_api_encoder, feature_names)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = joblib.load(os.path.join(model_dir, 'malware_classifier.joblib'))\n",
        "        tfidf = joblib.load(os.path.join(model_dir, 'tfidf_vectorizer.joblib'))\n",
        "        malware_encoder = joblib.load(os.path.join(model_dir, 'malware_type_encoder.joblib'))\n",
        "        first_api_encoder = joblib.load(os.path.join(model_dir, 'first_api_encoder.joblib'))\n",
        "        last_api_encoder = joblib.load(os.path.join(model_dir, 'last_api_encoder.joblib'))\n",
        "        feature_names = joblib.load(os.path.join(model_dir, 'feature_names.joblib'))\n",
        "\n",
        "        return model, tfidf, malware_encoder, first_api_encoder, last_api_encoder, feature_names\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading model artifacts: {str(e)}\")\n",
        "\n",
        "def prepare_inference_data(df, first_api_encoder, last_api_encoder, tfidf):\n",
        "    \"\"\"\n",
        "    Prepare new data for inference using extended API handling.\n",
        "    \"\"\"\n",
        "    # Handle API calls using extended encoders\n",
        "    first_api_encoded = first_api_encoder.transform(df['first_api'])\n",
        "    last_api_encoded = last_api_encoder.transform(df['last_api'])\n",
        "\n",
        "    # Normalize api_call_count\n",
        "    api_call_count_norm = np.log1p(df['api_call_count'])\n",
        "\n",
        "    # Transform API sequences using TF-IDF\n",
        "    # The vectorizer will now handle unseen tokens using the predefined vocabulary\n",
        "    api_sequence_features = tfidf.transform(df['api_sequence'])\n",
        "\n",
        "    # Combine features\n",
        "    numeric_features = np.column_stack((\n",
        "        first_api_encoded,\n",
        "        last_api_encoded,\n",
        "        api_call_count_norm\n",
        "    ))\n",
        "    numeric_features_sparse = csr_matrix(numeric_features)\n",
        "\n",
        "    return hstack([numeric_features_sparse, api_sequence_features])\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, output_path=None):\n",
        "    \"\"\"\n",
        "    Create and save a confusion matrix visualization.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True labels\n",
        "        y_pred (array-like): Predicted labels\n",
        "        class_names (array-like): List of class names\n",
        "        output_path (str, optional): Path to save the confusion matrix plot\n",
        "    \"\"\"\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate percentages\n",
        "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Create figure and axes\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm_percent, annot=True, fmt='.1f',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cmap='YlOrRd')\n",
        "\n",
        "    plt.title('Confusion Matrix (%)')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Rotate axis labels for better readability\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=45)\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if output_path:\n",
        "        plt.savefig(output_path)\n",
        "        print(f\"Confusion matrix plot saved to {output_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def process_single_file(model_dir, file_path, run_dir):\n",
        "    \"\"\"\n",
        "    Process a single CSV file and save results.\n",
        "\n",
        "    Args:\n",
        "        model_dir (str): Directory containing model artifacts\n",
        "        file_path (str): Path to input CSV file\n",
        "        run_dir (str): Directory for current run results\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame with predictions, dict with metrics)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    initial_metrics = get_system_metrics()\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nProcessing {file_name}...\")\n",
        "\n",
        "        # Load model and make predictions\n",
        "        model, tfidf, malware_encoder, first_api_encoder, last_api_encoder, feature_names = load_model_artifacts(model_dir)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Prepare features and make predictions\n",
        "        X = prepare_inference_data(df, first_api_encoder, last_api_encoder, tfidf)\n",
        "        predictions = model.predict(X)\n",
        "        prediction_probs = model.predict_proba(X)\n",
        "\n",
        "        # Add predictions to DataFrame\n",
        "        df['predicted_malware_type'] = malware_encoder.inverse_transform(predictions)\n",
        "        for i, class_name in enumerate(malware_encoder.classes_):\n",
        "            df[f'prob_{class_name}'] = prediction_probs[:, i]\n",
        "        df['confidence'] = prediction_probs.max(axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        file_metrics = {\n",
        "            'filename': file_name,\n",
        "            'processing_time': time.time() - start_time,\n",
        "            'sample_count': len(df),\n",
        "            'avg_confidence': df['confidence'].mean(),\n",
        "            'low_confidence_count': sum(df['confidence'] < 0.5)\n",
        "        }\n",
        "\n",
        "        if 'malware_type' in df.columns:\n",
        "            accuracy = np.mean(df['malware_type'] == df['predicted_malware_type'])\n",
        "            file_metrics['accuracy'] = round(accuracy, 4)\n",
        "\n",
        "            # Create confusion matrix\n",
        "            plot_confusion_matrix(\n",
        "                df['malware_type'],\n",
        "                df['predicted_malware_type'],\n",
        "                malware_encoder.classes_,\n",
        "                output_path=os.path.join(run_dir, 'confusion_matrices', f'confusion_matrix_{file_name}.png')\n",
        "            )\n",
        "\n",
        "        # Add system metrics\n",
        "        final_metrics = get_system_metrics()\n",
        "        file_metrics.update({\n",
        "            'peak_memory_mb': max(initial_metrics['memory_usage_mb'], final_metrics['memory_usage_mb']),\n",
        "            'peak_cpu_percent': max(initial_metrics['cpu_percent'], final_metrics['cpu_percent']),\n",
        "            'peak_threads': max(initial_metrics['threads'], final_metrics['threads'])\n",
        "        })\n",
        "\n",
        "        # Save individual results\n",
        "        output_path = os.path.join(run_dir, 'individual_results', f'predictions_{file_name}')\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Results saved to {output_path}\")\n",
        "\n",
        "        return df, file_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def process_multiple_files(model_dir, csv_paths, base_output_dir):\n",
        "    \"\"\"\n",
        "    Process multiple CSV files and generate combined analysis.\n",
        "\n",
        "    Args:\n",
        "        model_dir (str): Directory containing model artifacts\n",
        "        csv_paths (list): List of paths to CSV files\n",
        "        base_output_dir (str): Base directory for all runs\n",
        "    \"\"\"\n",
        "    # Create directory for this run\n",
        "    run_dir = create_run_directory(base_output_dir)\n",
        "    print(f\"Created new run directory: {run_dir}\")\n",
        "\n",
        "    # Process each file individually\n",
        "    all_metrics = []\n",
        "    all_dfs = []\n",
        "\n",
        "    for file_path in csv_paths:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: File not found - {file_path}\")\n",
        "            continue\n",
        "\n",
        "        df, metrics = process_single_file(model_dir, file_path, run_dir)\n",
        "\n",
        "        if df is not None and metrics is not None:\n",
        "            all_dfs.append(df)\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "    if not all_dfs:\n",
        "        raise ValueError(\"No files were processed successfully\")\n",
        "\n",
        "    # Process combined dataset\n",
        "    print(\"\\nProcessing combined dataset...\")\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    combined_metrics = {\n",
        "        'filename': 'combined_dataset',\n",
        "        'sample_count': len(combined_df),\n",
        "        'avg_confidence': combined_df['confidence'].mean(),\n",
        "        'low_confidence_count': sum(combined_df['confidence'] < 0.5)\n",
        "    }\n",
        "\n",
        "    if 'malware_type' in combined_df.columns:\n",
        "        accuracy = np.mean(combined_df['malware_type'] == combined_df['predicted_malware_type'])\n",
        "        combined_metrics['accuracy'] = round(accuracy, 4)\n",
        "\n",
        "        # Create combined confusion matrix\n",
        "        plot_confusion_matrix(\n",
        "            combined_df['malware_type'],\n",
        "            combined_df['predicted_malware_type'],\n",
        "            combined_df['predicted_malware_type'].unique(),\n",
        "            output_path=os.path.join(run_dir, 'combined_results', 'confusion_matrix_combined.png')\n",
        "        )\n",
        "\n",
        "    # Save combined results\n",
        "    combined_output = os.path.join(run_dir, 'combined_results', 'combined_predictions.csv')\n",
        "    combined_df.to_csv(combined_output, index=False)\n",
        "\n",
        "    # Create summary report\n",
        "    summary = pd.DataFrame(all_metrics + [combined_metrics])\n",
        "    summary.to_csv(os.path.join(run_dir, 'processing_metrics.csv'), index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nProcessing Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    for metrics in all_metrics:\n",
        "        print(f\"\\nFile: {metrics['filename']}\")\n",
        "        print(f\"Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\n",
        "        print(f\"Processing Time: {metrics['processing_time']:.2f} seconds\")\n",
        "        print(f\"Peak Memory Usage: {metrics['peak_memory_mb']:.2f} MB\")\n",
        "        print(f\"Peak CPU Usage: {metrics['peak_cpu_percent']:.2f}%\")\n",
        "\n",
        "    print(\"\\nCombined Dataset Results:\")\n",
        "    print(f\"Total Samples: {combined_metrics['sample_count']}\")\n",
        "    if 'accuracy' in combined_metrics:\n",
        "        print(f\"Overall Accuracy: {combined_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Average Confidence: {combined_metrics['avg_confidence']:.4f}\")\n",
        "\n",
        "    print(f\"\\nAll results saved in: {run_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specific CSV paths\n",
        "    csv_paths = [\n",
        "        \"/csv/path/dataset.csv\"\n",
        "    ]\n",
        "\n",
        "    model_dir = '/model/directory/'\n",
        "    base_output_dir = '/output/directory/'\n",
        "\n",
        "    process_multiple_files(model_dir, csv_paths, base_output_dir)"
      ],
      "metadata": {
        "id": "SBJMJeiELlXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "yrf20zf4KIQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "IFbcebfRMByj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import joblib\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "import psutil\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class ExtendedAPIEncoder:\n",
        "    \"\"\"\n",
        "    Custom encoder for API calls that handles unseen values using a predefined vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, unknown_value=-1):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.unknown_value = unknown_value\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def load_vocabulary(self, vocab_file):\n",
        "        \"\"\"Load API vocabulary from a text file\"\"\"\n",
        "        with open(vocab_file, 'r') as f:\n",
        "            api_calls = {line.strip() for line in f if line.strip()}\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def add_to_vocabulary(self, api_calls):\n",
        "        \"\"\"Add additional API calls to vocabulary\"\"\"\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def fit(self, api_calls):\n",
        "        \"\"\"Fit the encoder using both the vocabulary and training data\"\"\"\n",
        "        all_apis = list(self.vocabulary.union(set(api_calls)))\n",
        "        self.label_encoder.fit(all_apis)\n",
        "        return self\n",
        "\n",
        "    def transform(self, api_calls):\n",
        "        \"\"\"Transform API calls, handling unseen values gracefully\"\"\"\n",
        "        api_calls_clean = np.array(api_calls).copy()\n",
        "        mask = ~np.isin(api_calls_clean, self.label_encoder.classes_)\n",
        "        if mask.any():\n",
        "            unseen_apis = set(api_calls_clean[mask])\n",
        "            print(f\"Warning: Found {len(unseen_apis)} unseen API calls not in vocabulary.\")\n",
        "            api_calls_clean[mask] = self.label_encoder.classes_[0]\n",
        "\n",
        "        return self.label_encoder.transform(api_calls_clean)\n",
        "\n",
        "    def fit_transform(self, api_calls):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        self.fit(api_calls)\n",
        "        return self.transform(api_calls)\n",
        "\n",
        "    def inverse_transform(self, encoded_values):\n",
        "        \"\"\"Convert encoded values back to API calls\"\"\"\n",
        "        return self.label_encoder.inverse_transform(encoded_values)\n",
        "\n",
        "    def classes_(self):\n",
        "        \"\"\"Return the classes (API calls) known to the encoder\"\"\"\n",
        "        return self.label_encoder.classes_\n",
        "\n",
        "class SystemMonitor:\n",
        "    \"\"\"Monitor system resources during training\"\"\"\n",
        "    def __init__(self, interval=1.0):\n",
        "        self.interval = interval\n",
        "        self.cpu_percentages = []\n",
        "        self.memory_usage = []\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start monitoring\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.cpu_percentages = []\n",
        "        self.memory_usage = []\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Record current system stats\"\"\"\n",
        "        self.cpu_percentages.append(psutil.cpu_percent(interval=0.1))\n",
        "        self.memory_usage.append(psutil.Process().memory_info().rss / 1024 / 1024)  # MB\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop monitoring and return stats\"\"\"\n",
        "        self.end_time = time.time()\n",
        "        return {\n",
        "            'training_time_seconds': self.end_time - self.start_time,\n",
        "            'avg_cpu_percent': np.mean(self.cpu_percentages),\n",
        "            'max_cpu_percent': max(self.cpu_percentages),\n",
        "            'avg_memory_mb': np.mean(self.memory_usage),\n",
        "            'max_memory_mb': max(self.memory_usage)\n",
        "        }\n",
        "\n",
        "def load_and_combine_data(file_paths):\n",
        "    \"\"\"\n",
        "    Load and combine data from multiple CSV files,\n",
        "    handling large field sizes.\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): List of paths to CSV files containing malware data\n",
        "    \"\"\"\n",
        "    # Increase field size limit to handle large API sequences\n",
        "    maxInt = sys.maxsize\n",
        "    while True:\n",
        "        try:\n",
        "            csv.field_size_limit(maxInt)\n",
        "            break\n",
        "        except OverflowError:\n",
        "            maxInt = int(maxInt/10)\n",
        "\n",
        "    dfs = []\n",
        "    required_columns = ['first_api', 'last_api', 'api_call_count',\n",
        "                       'api_sequence', 'malware_type']\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            # Use the 'python' engine to handle potential parsing issues\n",
        "            df = pd.read_csv(file_path, engine='python')\n",
        "\n",
        "            # Validate required columns\n",
        "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "            if missing_columns:\n",
        "                raise ValueError(f\"File {file_path} is missing required columns: {missing_columns}\")\n",
        "\n",
        "            # Basic data cleaning\n",
        "            df = df.dropna()\n",
        "            df['api_call_count'] = df['api_call_count'].astype(int)\n",
        "            df['api_sequence'] = df['api_sequence'].astype(str)\n",
        "\n",
        "            # Add source file information\n",
        "            df['source_file'] = os.path.basename(file_path)\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "            print(f\"\\nLoaded data from {file_path}\")\n",
        "            print(f\"Samples: {len(df)}\")\n",
        "            print(\"Class distribution:\")\n",
        "            for malware_type, count in df['malware_type'].value_counts().items():\n",
        "                print(f\"{malware_type}: {count} samples\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Print combined dataset statistics\n",
        "    print(\"\\nCombined Dataset Statistics:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Total samples: {len(combined_df)}\")\n",
        "    print(\"\\nOverall class distribution:\")\n",
        "    class_dist = combined_df['malware_type'].value_counts()\n",
        "    for malware_type, count in class_dist.items():\n",
        "        print(f\"{malware_type}: {count} samples\")\n",
        "\n",
        "    # API calls statistics\n",
        "    print(\"\\nAPI calls statistics:\")\n",
        "    api_stats = combined_df['api_call_count'].describe()\n",
        "    print(f\"Min API calls: {api_stats['min']:.0f}\")\n",
        "    print(f\"Max API calls: {api_stats['max']:.0f}\")\n",
        "    print(f\"Mean API calls: {api_stats['mean']:.0f}\")\n",
        "    print(f\"Median API calls: {api_stats['50%']:.0f}\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def prepare_data(df, api_vocab_file, max_features=2000, use_smote=True):\n",
        "    \"\"\"\n",
        "    Prepare the data using extended API vocabulary.\n",
        "    \"\"\"\n",
        "    first_api_encoder = ExtendedAPIEncoder()\n",
        "    last_api_encoder = ExtendedAPIEncoder()\n",
        "    malware_type_encoder = LabelEncoder()\n",
        "\n",
        "    print(\"Loading API vocabulary...\")\n",
        "    first_api_encoder.load_vocabulary(api_vocab_file)\n",
        "    last_api_encoder.load_vocabulary(api_vocab_file)\n",
        "\n",
        "    print(\"Encoding API calls...\")\n",
        "    df['first_api_encoded'] = first_api_encoder.fit_transform(df['first_api'])\n",
        "    df['last_api_encoded'] = last_api_encoder.fit_transform(df['last_api'])\n",
        "    df['malware_type_encoded'] = malware_type_encoder.fit_transform(df['malware_type'])\n",
        "\n",
        "    df['api_call_count_norm'] = np.log1p(df['api_call_count'])\n",
        "\n",
        "    print(\"Creating TF-IDF features...\")\n",
        "    with open(api_vocab_file, 'r') as f:\n",
        "        vocabulary = {line.strip() for line in f if line.strip()}\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        sublinear_tf=True,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=1,\n",
        "        vocabulary=vocabulary\n",
        "    )\n",
        "    api_sequence_features = tfidf.fit_transform(df['api_sequence'])\n",
        "\n",
        "    numeric_features = np.column_stack((\n",
        "        df['first_api_encoded'],\n",
        "        df['last_api_encoded'],\n",
        "        df['api_call_count_norm']\n",
        "    ))\n",
        "    numeric_features_sparse = csr_matrix(numeric_features)\n",
        "\n",
        "    X = hstack([numeric_features_sparse, api_sequence_features])\n",
        "    y = df['malware_type_encoded']\n",
        "\n",
        "    if use_smote:\n",
        "        print(\"\\nApplying SMOTE to balance classes...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X, y = smote.fit_resample(X, y)\n",
        "        print(f\"Shape after SMOTE: {X.shape}\")\n",
        "\n",
        "    feature_names = (['First API', 'Last API', 'API Call Count'] +\n",
        "                    [f'API_Seq_{i}' for i in range(api_sequence_features.shape[1])])\n",
        "\n",
        "    return (X, y, malware_type_encoder, first_api_encoder, last_api_encoder,\n",
        "            tfidf, feature_names)\n",
        "\n",
        "\n",
        "def format_classification_report(y_true, y_pred, target_names):\n",
        "    \"\"\"Generate formatted classification report with 4 decimal precision\"\"\"\n",
        "    report_dict = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
        "\n",
        "    formatted_report = {\"precision\": {}, \"recall\": {}, \"f1-score\": {}, \"support\": {}}\n",
        "\n",
        "    for class_name in target_names:\n",
        "        metrics = report_dict[class_name]\n",
        "        formatted_report[\"precision\"][class_name] = f\"{metrics['precision']:.4f}\"\n",
        "        formatted_report[\"recall\"][class_name] = f\"{metrics['recall']:.4f}\"\n",
        "        formatted_report[\"f1-score\"][class_name] = f\"{metrics['f1-score']:.4f}\"\n",
        "        formatted_report[\"support\"][class_name] = int(metrics['support'])\n",
        "\n",
        "    # Add weighted averages\n",
        "    for avg_type in ['macro avg', 'weighted avg']:\n",
        "        metrics = report_dict[avg_type]\n",
        "        formatted_report[\"precision\"][avg_type] = f\"{metrics['precision']:.4f}\"\n",
        "        formatted_report[\"recall\"][avg_type] = f\"{metrics['recall']:.4f}\"\n",
        "        formatted_report[\"f1-score\"][avg_type] = f\"{metrics['f1-score']:.4f}\"\n",
        "        formatted_report[\"support\"][avg_type] = int(metrics['support'])\n",
        "\n",
        "    return formatted_report\n",
        "\n",
        "def train_xgboost(X, y, num_class, system_monitor=None):\n",
        "    \"\"\"\n",
        "    Enhanced XGBoost training with system monitoring and early stopping\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        min_child_weight=1,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective='multi:softprob',\n",
        "        num_class=num_class,\n",
        "        tree_method='hist',\n",
        "        eval_metric=['mlogloss', 'merror'],\n",
        "        early_stopping_rounds=20,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    cv_scores = []\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        eval_set = [(X_val, y_val)]\n",
        "\n",
        "        xgb_model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=eval_set,\n",
        "            verbose=100\n",
        "        )\n",
        "\n",
        "        score = xgb_model.score(X_val, y_val)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "        fold_time = time.time() - fold_start_time\n",
        "        print(f\"Fold {fold} accuracy: {score:.4f} (Time: {fold_time:.2f}s)\")\n",
        "\n",
        "        if system_monitor:\n",
        "            system_monitor.update()\n",
        "\n",
        "    print(f\"\\nMean CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Final training on full dataset\n",
        "    eval_set = [(X, y)]\n",
        "    xgb_model.fit(X, y, eval_set=eval_set, verbose=100)\n",
        "\n",
        "    return xgb_model\n",
        "\n",
        "def evaluate_model(model, X, y, malware_type_encoder, feature_names, output_dir, system_monitor):\n",
        "    \"\"\"\n",
        "    Enhanced evaluation with system metrics and precise formatting\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    class_names = malware_type_encoder.classes_\n",
        "\n",
        "    # Generate formatted classification report\n",
        "    formatted_report = format_classification_report(y_test, y_pred, class_names)\n",
        "\n",
        "    # Save metrics to JSON\n",
        "    metrics_file = os.path.join(output_dir, 'model_metrics.json')\n",
        "    system_stats = system_monitor.stop()\n",
        "\n",
        "    metrics_data = {\n",
        "        'classification_metrics': formatted_report,\n",
        "        'system_stats': system_stats,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "    # Print formatted results\n",
        "    print(\"\\nClassification Metrics (4 decimal precision):\")\n",
        "    print(\"-\" * 80)\n",
        "    for metric in [\"precision\", \"recall\", \"f1-score\"]:\n",
        "        print(f\"\\n{metric.upper()}:\")\n",
        "        for class_name, value in formatted_report[metric].items():\n",
        "            print(f\"{class_name}: {value}\")\n",
        "\n",
        "    print(\"\\nSystem Statistics:\")\n",
        "    print(f\"Total training time: {system_stats['training_time_seconds']:.2f} seconds\")\n",
        "    print(f\"Average CPU usage: {system_stats['avg_cpu_percent']:.1f}%\")\n",
        "    print(f\"Peak CPU usage: {system_stats['max_cpu_percent']:.1f}%\")\n",
        "    print(f\"Average memory usage: {system_stats['avg_memory_mb']:.1f} MB\")\n",
        "    print(f\"Peak memory usage: {system_stats['max_memory_mb']:.1f} MB\")\n",
        "\n",
        "    # Create confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance plot\n",
        "    n_top_features = 20\n",
        "    importance_type = 'weight'\n",
        "    importance_scores = model.get_booster().get_score(importance_type=importance_type)\n",
        "\n",
        "    feature_importance_list = [(feature, importance_scores.get(feature, 0))\n",
        "                              for feature in feature_names]\n",
        "\n",
        "    feature_importance = pd.DataFrame(feature_importance_list, columns=['feature', 'importance'])\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "    feature_importance = feature_importance.head(n_top_features)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "    plt.title(f'Top {n_top_features} Most Important Features ({importance_type})')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'feature_importance.png'))\n",
        "    plt.close()\n",
        "\n",
        "def save_artifacts(output_dir, model, tfidf, malware_type_encoder,\n",
        "                  first_api_encoder, last_api_encoder, feature_names):\n",
        "    \"\"\"\n",
        "    Save all model artifacts required for inference.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    artifact_dict = {\n",
        "        'malware_classifier.json': lambda x: x.save_model(os.path.join(output_dir, 'malware_classifier.json')),\n",
        "        'tfidf_vectorizer.joblib': lambda x: joblib.dump(x, os.path.join(output_dir, 'tfidf_vectorizer.joblib')),\n",
        "        'malware_type_encoder.joblib': lambda x: joblib.dump(x, os.path.join(output_dir, 'malware_type_encoder.joblib')),\n",
        "        'first_api_encoder.joblib': lambda x: joblib.dump(x, os.path.join(output_dir, 'first_api_encoder.joblib')),\n",
        "        'last_api_encoder.joblib': lambda x: joblib.dump(x, os.path.join(output_dir, 'last_api_encoder.joblib')),\n",
        "        'feature_names.joblib': lambda x: joblib.dump(x, os.path.join(output_dir, 'feature_names.joblib'))\n",
        "    }\n",
        "\n",
        "    artifacts = [model, tfidf, malware_type_encoder, first_api_encoder, last_api_encoder, feature_names]\n",
        "\n",
        "    for (filename, save_func), artifact in zip(artifact_dict.items(), artifacts):\n",
        "        try:\n",
        "            save_func(artifact)\n",
        "            print(f\"Saved {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {filename}: {str(e)}\")\n",
        "\n",
        "def main(csv_paths, output_dir, api_vocab_file):\n",
        "    \"\"\"\n",
        "    Enhanced main execution function with system monitoring\n",
        "    \"\"\"\n",
        "    system_monitor = SystemMonitor()\n",
        "    system_monitor.start()\n",
        "\n",
        "    # Load and prepare data\n",
        "    df = load_and_combine_data(csv_paths)\n",
        "    (X, y, malware_type_encoder, first_api_encoder, last_api_encoder,\n",
        "     tfidf, feature_names) = prepare_data(df, api_vocab_file, max_features=2000, use_smote=True)\n",
        "\n",
        "    # Get number of classes for XGBoost\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Train model with system monitoring\n",
        "    model = train_xgboost(X, y, num_classes, system_monitor)\n",
        "\n",
        "    # Evaluate model with system metrics\n",
        "    evaluate_model(model, X, y, malware_type_encoder, feature_names, output_dir, system_monitor)\n",
        "\n",
        "    # Save artifacts\n",
        "    save_artifacts(\n",
        "        output_dir,\n",
        "        model,\n",
        "        tfidf,\n",
        "        malware_type_encoder,\n",
        "        first_api_encoder,\n",
        "        last_api_encoder,\n",
        "        feature_names\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining complete. All artifacts saved to {output_dir}\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_paths = [\n",
        "        \"/csv/path/data.csv\"\n",
        "    ]\n",
        "    output_dir = \"output/directory/\"\n",
        "    api_vocab_file = \"/api/calls/path/windowsapicalls.txt\"\n",
        "    model = main(csv_paths, output_dir, api_vocab_file)"
      ],
      "metadata": {
        "id": "7GHMIXZAMLXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "mal8IYUgMPYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier, Booster, DMatrix\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "from datetime import datetime\n",
        "\n",
        "class ExtendedAPIEncoder:\n",
        "    \"\"\"\n",
        "    Custom encoder for API calls that handles unseen values using a predefined vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, unknown_value=-1):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.unknown_value = unknown_value\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def load_vocabulary(self, vocab_file):\n",
        "        \"\"\"Load API vocabulary from a text file\"\"\"\n",
        "        with open(vocab_file, 'r') as f:\n",
        "            api_calls = {line.strip() for line in f if line.strip()}\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def add_to_vocabulary(self, api_calls):\n",
        "        \"\"\"Add additional API calls to vocabulary\"\"\"\n",
        "        self.vocabulary.update(api_calls)\n",
        "\n",
        "    def fit(self, api_calls):\n",
        "        \"\"\"Fit the encoder using both the vocabulary and training data\"\"\"\n",
        "        all_apis = list(self.vocabulary.union(set(api_calls)))\n",
        "        self.label_encoder.fit(all_apis)\n",
        "        return self\n",
        "\n",
        "    def transform(self, api_calls):\n",
        "        \"\"\"Transform API calls, handling unseen values gracefully\"\"\"\n",
        "        api_calls_clean = np.array(api_calls).copy()\n",
        "        mask = ~np.isin(api_calls_clean, self.label_encoder.classes_)\n",
        "        if mask.any():\n",
        "            unseen_apis = set(api_calls_clean[mask])\n",
        "            print(f\"Warning: Found {len(unseen_apis)} unseen API calls not in vocabulary.\")\n",
        "            api_calls_clean[mask] = self.label_encoder.classes_[0]\n",
        "\n",
        "        return self.label_encoder.transform(api_calls_clean)\n",
        "\n",
        "    def fit_transform(self, api_calls):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        self.fit(api_calls)\n",
        "        return self.transform(api_calls)\n",
        "\n",
        "    def inverse_transform(self, encoded_values):\n",
        "        \"\"\"Convert encoded values back to API calls\"\"\"\n",
        "        return self.label_encoder.inverse_transform(encoded_values)\n",
        "\n",
        "    def classes_(self):\n",
        "        \"\"\"Return the classes (API calls) known to the encoder\"\"\"\n",
        "        return self.label_encoder.classes_\n",
        "\n",
        "def create_run_directory(base_output_dir):\n",
        "    \"\"\"\n",
        "    Create a timestamped directory for the current run.\n",
        "\n",
        "    Args:\n",
        "        base_output_dir (str): Base directory for all runs\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the newly created directory\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    run_dir = os.path.join(base_output_dir, f'run_{timestamp}')\n",
        "\n",
        "    # Create directory structure\n",
        "    subdirs = ['individual_results', 'confusion_matrices', 'combined_results', 'metrics']\n",
        "    for subdir in subdirs:\n",
        "        os.makedirs(os.path.join(run_dir, subdir), exist_ok=True)\n",
        "\n",
        "    return run_dir\n",
        "\n",
        "def get_system_metrics():\n",
        "    \"\"\"\n",
        "    Collect system metrics during model execution.\n",
        "    \"\"\"\n",
        "    process = psutil.Process()\n",
        "    return {\n",
        "        'memory_usage_mb': process.memory_info().rss / 1024 / 1024,\n",
        "        'cpu_percent': process.cpu_percent(),\n",
        "        'threads': process.num_threads(),\n",
        "    }\n",
        "\n",
        "def load_model_artifacts(model_dir):\n",
        "    \"\"\"\n",
        "    Load all saved model artifacts required for inference.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load XGBoost model using lower-level API to avoid version issues\n",
        "        booster = Booster()\n",
        "        booster.load_model(os.path.join(model_dir, 'malware_classifier.json'))\n",
        "\n",
        "        # Create XGBClassifier wrapper\n",
        "        model = XGBClassifier()\n",
        "        model._Booster = booster\n",
        "        model.n_classes_ = len(booster.get_dump())  # Set number of classes\n",
        "\n",
        "        # Load other artifacts\n",
        "        tfidf = joblib.load(os.path.join(model_dir, 'tfidf_vectorizer.joblib'))\n",
        "        malware_type_encoder = joblib.load(os.path.join(model_dir, 'malware_type_encoder.joblib'))\n",
        "        first_api_encoder = joblib.load(os.path.join(model_dir, 'first_api_encoder.joblib'))\n",
        "        last_api_encoder = joblib.load(os.path.join(model_dir, 'last_api_encoder.joblib'))\n",
        "\n",
        "        # Set objective for XGBClassifier\n",
        "        model.objective = 'multi:softprob'\n",
        "\n",
        "        return model, tfidf, malware_type_encoder, first_api_encoder, last_api_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model artifacts: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def prepare_batch_samples(df, tfidf, first_api_encoder, last_api_encoder):\n",
        "    \"\"\"\n",
        "    Prepare multiple samples for prediction.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode first and last API calls\n",
        "        first_api_encoded = first_api_encoder.transform(df['first_api'])\n",
        "        last_api_encoded = last_api_encoder.transform(df['last_api'])\n",
        "\n",
        "        # Normalize API call count\n",
        "        api_call_count_norm = np.log1p(df['api_call_count'])\n",
        "\n",
        "        # Create TF-IDF features for API sequence\n",
        "        api_sequence_features = tfidf.transform(df['api_sequence'])\n",
        "\n",
        "        # Combine features\n",
        "        numeric_features = np.column_stack((\n",
        "            first_api_encoded,\n",
        "            last_api_encoded,\n",
        "            api_call_count_norm\n",
        "        ))\n",
        "        numeric_features_sparse = csr_matrix(numeric_features)\n",
        "\n",
        "        # Create final feature matrix\n",
        "        X = hstack([numeric_features_sparse, api_sequence_features])\n",
        "\n",
        "        return X\n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing features: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predicted_labels, class_names, output_path_prefix):\n",
        "    \"\"\"\n",
        "    Create and save confusion matrix visualization.\n",
        "\n",
        "    Args:\n",
        "        output_path_prefix (str): Base path for saving confusion matrices (without extension)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(true_labels, predicted_labels)\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Raw counts matrix\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names,\n",
        "                    yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix (Raw Counts)')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_path_prefix}_counts.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Normalized matrix\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.4%', cmap='Blues',\n",
        "                    xticklabels=class_names,\n",
        "                    yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix (Normalized)')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{output_path_prefix}_normalized.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating confusion matrix: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def process_single_file(model_dir, file_path, run_dir):\n",
        "    \"\"\"\n",
        "    Process a single CSV file and save results.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    initial_metrics = get_system_metrics()\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nProcessing {file_name}...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(file_path)\n",
        "        required_columns = ['first_api', 'last_api', 'api_call_count', 'api_sequence', 'malware_type']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
        "\n",
        "        # Load model and artifacts\n",
        "        model, tfidf, malware_type_encoder, first_api_encoder, last_api_encoder = \\\n",
        "            load_model_artifacts(model_dir)\n",
        "\n",
        "        # Prepare features\n",
        "        X = prepare_batch_samples(df, tfidf, first_api_encoder, last_api_encoder)\n",
        "\n",
        "        # Make predictions\n",
        "        dmatrix = DMatrix(X)\n",
        "        pred_proba = model.get_booster().predict(dmatrix)\n",
        "        pred_classes = pred_proba.argmax(axis=1)\n",
        "        predicted_labels = malware_type_encoder.inverse_transform(pred_classes)\n",
        "\n",
        "        # Prepare results DataFrame\n",
        "        results_df = df.copy()\n",
        "        results_df['predicted_malware_type'] = predicted_labels\n",
        "        results_df['confidence'] = [max(probs) for probs in pred_proba]\n",
        "\n",
        "        # Add class probabilities\n",
        "        for i, class_name in enumerate(malware_type_encoder.classes_):\n",
        "            results_df[f'prob_{class_name}'] = pred_proba[:, i]\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = (predicted_labels == df['malware_type']).mean()\n",
        "        file_metrics = {\n",
        "            'filename': file_name,\n",
        "            'processing_time': time.time() - start_time,\n",
        "            'sample_count': len(df),\n",
        "            'accuracy': round(accuracy, 4),\n",
        "            'avg_confidence': results_df['confidence'].mean(),\n",
        "            'low_confidence_count': sum(results_df['confidence'] < 0.5)\n",
        "        }\n",
        "\n",
        "        # Create confusion matrix\n",
        "        output_path_prefix = os.path.join(run_dir, 'confusion_matrices', f'confusion_matrix_{file_name[:-4]}')\n",
        "        plot_confusion_matrix(\n",
        "            df['malware_type'],\n",
        "            predicted_labels,\n",
        "            malware_type_encoder.classes_,\n",
        "            output_path_prefix\n",
        "        )\n",
        "\n",
        "        # Save classification report\n",
        "        report = classification_report(df['malware_type'], predicted_labels, output_dict=True)\n",
        "        report_df = pd.DataFrame(report).transpose()\n",
        "        report_df.to_csv(os.path.join(run_dir, 'metrics', f'classification_report_{file_name}'), index=True)\n",
        "\n",
        "        # Add system metrics\n",
        "        final_metrics = get_system_metrics()\n",
        "        file_metrics.update({\n",
        "            'peak_memory_mb': max(initial_metrics['memory_usage_mb'], final_metrics['memory_usage_mb']),\n",
        "            'peak_cpu_percent': max(initial_metrics['cpu_percent'], final_metrics['cpu_percent']),\n",
        "            'peak_threads': max(initial_metrics['threads'], final_metrics['threads'])\n",
        "        })\n",
        "\n",
        "        # Save individual results\n",
        "        output_path = os.path.join(run_dir, 'individual_results', f'predictions_{file_name}')\n",
        "        results_df.to_csv(output_path, index=False)\n",
        "        print(f\"Results saved to {output_path}\")\n",
        "\n",
        "        return results_df, file_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def process_multiple_files(model_dir, csv_paths, base_output_dir):\n",
        "    \"\"\"\n",
        "    Process multiple CSV files and generate combined analysis.\n",
        "    \"\"\"\n",
        "    # Create directory for this run\n",
        "    run_dir = create_run_directory(base_output_dir)\n",
        "    print(f\"Created new run directory: {run_dir}\")\n",
        "\n",
        "    # Process each file individually\n",
        "    all_metrics = []\n",
        "    all_dfs = []\n",
        "\n",
        "    for file_path in csv_paths:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: File not found - {file_path}\")\n",
        "            continue\n",
        "\n",
        "        df, metrics = process_single_file(model_dir, file_path, run_dir)\n",
        "\n",
        "        if df is not None and metrics is not None:\n",
        "            all_dfs.append(df)\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "    if not all_dfs:\n",
        "        raise ValueError(\"No files were processed successfully\")\n",
        "\n",
        "    # Process combined dataset\n",
        "    print(\"\\nProcessing combined dataset...\")\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    # Calculate combined metrics\n",
        "    combined_metrics = {\n",
        "        'filename': 'combined_dataset',\n",
        "        'sample_count': len(combined_df),\n",
        "        'accuracy': round((combined_df['malware_type'] == combined_df['predicted_malware_type']).mean(), 4),\n",
        "        'avg_confidence': combined_df['confidence'].mean(),\n",
        "        'low_confidence_count': sum(combined_df['confidence'] < 0.5)\n",
        "    }\n",
        "\n",
        "    # Create combined confusion matrix\n",
        "    plot_confusion_matrix(\n",
        "        combined_df['malware_type'],\n",
        "        combined_df['predicted_malware_type'],\n",
        "        combined_df['predicted_malware_type'].unique(),\n",
        "        os.path.join(run_dir, 'combined_results', 'confusion_matrix_combined')\n",
        "    )\n",
        "\n",
        "    # Save combined results\n",
        "    combined_output = os.path.join(run_dir, 'combined_results', 'combined_predictions.csv')\n",
        "    combined_df.to_csv(combined_output, index=False)\n",
        "\n",
        "    # Create summary report\n",
        "    summary = pd.DataFrame(all_metrics + [combined_metrics])\n",
        "    summary.to_csv(os.path.join(run_dir, 'metrics', 'processing_metrics.csv'), index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nProcessing Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    for metrics in all_metrics:\n",
        "        print(f\"\\nFile: {metrics['filename']}\")\n",
        "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"Processing Time: {metrics['processing_time']:.2f} seconds\")\n",
        "        print(f\"Peak Memory Usage: {metrics['peak_memory_mb']:.2f} MB\")\n",
        "        print(f\"Peak CPU Usage: {metrics['peak_cpu_percent']:.2f}%\")\n",
        "\n",
        "    print(\"\\nCombined Dataset Results:\")\n",
        "    print(f\"Total Samples: {combined_metrics['sample_count']}\")\n",
        "    print(f\"Overall Accuracy: {combined_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Average Confidence: {combined_metrics['avg_confidence']:.4f}\")\n",
        "\n",
        "    print(f\"\\nAll results saved in: {run_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specific CSV paths\n",
        "    csv_paths = [\n",
        "        \"/csv/path/dataset.csv\"\n",
        "    ]\n",
        "\n",
        "    model_dir = '/model/directory/'\n",
        "    base_output_dir = '/output/directory/'\n",
        "\n",
        "    process_multiple_files(model_dir, csv_paths, base_output_dir)"
      ],
      "metadata": {
        "id": "H_N18U81MZQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
